{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNHtKN7MSabC",
        "collapsed": true,
        "outputId": "018892db-6f4a-4339-f6ed-bb4caef714a2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 18 22:39:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gpu_tpu_lab_server"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuVkqbyIWDHA",
        "outputId": "984321ad-ea0e-487a-c05c-b83c533cf173"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpu_tpu_lab_server\n",
            "  Downloading gpu_tpu_lab_server-0.0.6-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gpu_tpu_lab_server) (21.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from gpu_tpu_lab_server) (21.1.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gpu_tpu_lab_server) (3.0.6)\n",
            "Installing collected packages: gpu-tpu-lab-server\n",
            "Successfully installed gpu-tpu-lab-server-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpu_tpu_lab_server.core import jupyter_lab, vscode, run_process_in_background, run_process_in_foreground"
      ],
      "metadata": {
        "id": "st8enh3VWPfm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vscode()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7AOfhCmWVS2",
        "outputId": "2f7beb7e-9c8e-418e-9b1b-3ccb98aceabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "https://fastdaima.loca.lt/?folder=/content/drive/MyDrive/colab\n",
            "b'[2022-01-18T22:41:27.723Z] info  Wrote default config file to ~/.config/code-server/config.yaml'\n",
            "b'[2022-01-18T22:41:28.331Z] info  code-server 3.10.2 387b12ef4ca404ffd39d84834e1f0776e9e3c005'\n",
            "b'[2022-01-18T22:41:28.333Z] info  Using user-data-dir /content/drive/MyDrive/colab/.vscode'\n",
            "b'[2022-01-18T22:41:28.366Z] info  Using config file ~/.config/code-server/config.yaml'\n",
            "b'[2022-01-18T22:41:28.366Z] info  HTTP server listening on http://127.0.0.1:9000'\n",
            "b'[2022-01-18T22:41:28.366Z] info    - Authentication is disabled'\n",
            "b'[2022-01-18T22:41:28.366Z] info    - Not serving HTTPS'\n",
            "b'npx: installed 22 in 3.215s'\n",
            "b'your url is: https://hot-otter-95.loca.lt'\n"
          ]
        }
      ]
    }
  ]
}